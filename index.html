<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Alessandro Simoni</title>
    <meta name="author" content="Alessandro Simoni">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link href='http://fonts.googleapis.com/css?family=Roboto' rel='stylesheet' type='text/css'>
    <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&amp;family=Roboto+Mono&amp;family=Roboto:wght@400;700&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&amp;family=Roboto+Mono&amp;family=Roboto:wght@400;700&amp;display=swap">
    <!--<link rel="icon" type="image/png" href="my_icon.png">-->
</head>

<body>

  <div class="container">

    <div class="header">
        <img class="pic" style="border-radius:50%; width: 200px; height: 200px;" src="images/AlessandroSimoni.jpg" alt=""/>
    </div>

    <div class="header" style="margin-top:-10px;">
        <div class="description">
            <div class="name">Alessandro Simoni</div>
            <div class="name" style="font-size: 18px; font-family: montserrat,sans-serif; color: #ffffff8a">
                Deep Learning & Computer Vision Engineer @ <a href="https://www.covisionlab.com/en" style="font-size: 18px;" target="_blank">Covision Lab</a>
            </div>
            
            <p class="contact">
                <br>
                <strong>Job offer info:</strong>&nbsp
                <a href="mailto:alle.simoni.as@gmail.com">Email</a> &nbsp|&nbsp
                <!--<a href="data/AlessandroSimoni-resume.pdf" target="_blank">Resum√©</a> &nbsp|&nbsp-->
                <a href="data/AlessandroSimoni-CV.pdf" target="_blank">CV</a> 
                <br><br>
                <strong>Social:</strong>&nbsp
                <a href="https://scholar.google.com/citations?user=MDGDMWIAAAAJ&hl=it" target="_blank">Google Scholar</a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/alessandro-simoni/" target="_blank">LinkedIn</a> &nbsp|&nbsp
                <a href="https://twitter.com/alle_simoni/" target="_blank">Twitter</a> &nbsp|&nbsp
                <a href="https://github.com/alexj94" target="_blank">Github</a>
            </p>

            <div class="about">About me</div>
            <p align="justify">
                My name is Alessandro Simoni. I am a dedicated <u><strong>Deep Learning and Computer Vision Engineer</strong></u>. 
                I got my Ph.D. in Computer Science from the University of Modena and Reggio Emilia in April 2024. 
                With experience in 3D Human-Centric Scene Understanding and 3D Object Reconstruction, 
                I have a strong passion for research and advancing the field of Deep Learning.
            </p>

            <div class="about">Education</div>
            <li style="font-family: montserrat,sans-serif; color: #ffffff8a; margin-top: 10px;">
                PhD in AI and Computer Vision, University of Modena and Reggio Emilia, April 2024
            </li>
            <li style="font-family: montserrat,sans-serif; color: #ffffff8a; margin-top: 10px;">
                MS in Computer Engineering, University of Modena and Reggio Emilia, February 2020
            </li>
            <li style="font-family: montserrat,sans-serif; color: #ffffff8a; margin-top: 10px;">
                BS in Computer Engineering, University of Modena and Reggio Emilia, February 2017
            </li>
        </div>
    </div>

<!-- Research -->
    <div class="research">
      <div class="header">Research activities</div>
      <div class="description">
          <h2 style="font-family: montserrat, sans-serif; font-weight: normal; margin: 10px 0;"><u>Authored publications:</u></h2>
      </div>
    </div>

<!-- RELEVANT paper list begins -->
    <div class="papers">

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/3d_rpe.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://arxiv.org/abs/2207.02519" class="title" target="_blank"><papertitle>Semi-Perspective Decoupled Heatmaps for 3D Robot Pose Estimation from Depth Maps</papertitle></a>
                </p>
                <p class="pub-descr">
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>
                </p>
                <p class="pub-descr">
                    <em>IROS 2022 + IEEE Robotics and Automation Letters</em> - <strong style="color: rgb(255, 0, 0)">Oral</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://arxiv.org/abs/2207.02519" target="_blank">arXiv</a> &nbsp|&nbsp
                    <a href="biblio/spdh.bib" target="_blank">bibtex</a> &nbsp|&nbsp
                    <a href="https://aimagelab.ing.unimore.it/go/rpe" target="_blank">webpage</a> &nbsp|&nbsp
                    <a href="https://aimagelab.ing.unimore.it/go/simba" target="_blank">dataset</a> &nbsp|&nbsp
                    <a href="https://github.com/aimagelab/rpe_spdh" target="_blank">code</a> &nbsp|&nbsp
                    <a href="data/spdh_slides.pdf" target="_blank">slides</a>
                </p>
                <p align="justify">Thanks to a novel 3D pose representation composed of two decoupled heatmaps, 
                    efficient deep networks from the 2D HPE domain can be adapted to accurately compute 3D joints locations in world coordinates. 
                    Moreover, depth maps are used to bridge the gap between synthetic and real data.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/mcmr.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://ieeexplore.ieee.org/abstract/document/9665934" class="title" target="_blank"><papertitle>Multi-Category Mesh Reconstruction From Image Collections</papertitle></a>
                </p>
                <p class="pub-descr">
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>
                </p>
                <p class="pub-descr">
                    <em>3DV 2021</em> - <strong style="color: green">Poster</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://arxiv.org/abs/2110.11256" target="_blank">arXiv</a> &nbsp|&nbsp
                    <a href="biblio/mcmr.bib" target="_blank">bibtex</a> &nbsp|&nbsp
                    <a href="https://github.com/aimagelab/mcmr" target="_blank">code</a> &nbsp|&nbsp
                    <a href="data/mcmr_poster.pdf" target="_blank">poster</a> &nbsp|&nbsp
                    <a href="data/mcmr_slides.pdf" target="_blank">slides</a> &nbsp|&nbsp
                    <a href="https://slideslive.com/38972381/multicategory-mesh-reconstruction-from-image-collections?ref=" target="_blank">presentation (video)</a>
                </p>
                <p align="justify">A multi-category mesh reconstruction framework infers the textured mesh of objects, 
                    learning category-specific priors in an unsupervised manner and obtaining smooth shapes with a dynamic mesh subdivision approach.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/cmc_vkl.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://www.scitepress.org/PublicationsDetail.aspx?ID=pr1Lw6QeqOY=&t=1" class="title" target="_blank"><papertitle>Improving Car Model Classification through Vehicle Keypoint Localization</papertitle></a>
                </p>
                <p class="pub-descr">
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>
                </p>
                <p class="pub-descr">
                    <em>VISAPP 2021</em> - <strong style="color: red">Oral</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://www.scitepress.org/Papers/2021/102078/102078.pdf" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/cmc_vkl.bib" target="_blank">bibtex</a> &nbsp|&nbsp
                    <a href="data/cmc_vkl_slides.pdf" target="_blank">slides</a> &nbsp|&nbsp
                    <a href="data/cmc_vkl_presentation.mp4" target="_blank">presentation (video)</a>
                </p>
                <p align="justify">A multi-task framework combines visual features and keypoint localization features in order to improve car model classification accuracy.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/fus_gen.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://ieeexplore.ieee.org/abstract/document/9412880" class="title" target="_blank"><papertitle>Future Urban Scenes Generation Through Vehicles Synthesis</papertitle></a>
                </p>
                <p class="pub-descr">
                <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=101" target="_blank">Luca Bergamini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=91" target="_blank">Andrea Palazzi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=38" target="_blank">Simone Calderara</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>
                </p>
                <p class="pub-descr">
                    <em>ICPR 2020</em> - <strong style="color: green">Poster</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://arxiv.org/abs/2007.00323" target="_blank">arXiv</a> &nbsp|&nbsp
                    <a href="biblio/fus_gen.bib" target="_blank">bibtex</a> &nbsp|&nbsp
                    <a href="https://github.com/alexj94/future_urban_scene_generation" target="_blank">code</a> &nbsp|&nbsp
                    <a href="data/fus_gen_poster.pdf" target="_blank">poster</a> &nbsp|&nbsp
                    <a href="data/fus_gen_slides.pdf" target="_blank">slides</a> &nbsp|&nbsp
                    <a href="data/fus_gen_presentation.mp4" target="_blank">presentation (video)</a>
                </p>
                <p align="justify">A two-stage approach in which interpretable information are exploited by a novel view synthesis architecture in order to 
                    reproduce the future visual appearance of vehicles in an urban scene.</p>
            </div>
        </div>
    </div>
<!-- RELEVANT paper list ends -->

    <div class="research">
        <div class="description" style="padding: 2.5% 2.5% 2.5% 2.5%;">
            <h2 style="font-family: montserrat, sans-serif; font-weight: normal; margin: 10px 0;"><u>Co-authored publications:</u></h2>
        </div>
    </div>

<!-- OTHER paper list begins -->

    <div class="papers">

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/carpatch.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="" class="title" target="_blank"><papertitle>CarPatch: A Synthetic Benchmark for Radiance Field Evaluation on Vehicle Components</papertitle></a>
                </p>
                <p class="pub-descr">
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=166" target="_blank">Davide Di Nucci</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://it.linkedin.com/in/matteo-tomei-86b9b9a6" target="_blank">Matteo Tomei</a>,
                    <a href="https://it.linkedin.com/in/luca-ciuffreda" target="_blank">Luca Ciuffreda</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>
                </p>
                <p class="pub-descr">
                    <em>ICIAP 2023</em> - <strong style="color: red">Oral</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://arxiv.org/abs/2307.12718" target="_blank">arXiv</a> &nbsp|&nbsp
                    <a href="https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=55" target="_blank">dataset</a>
                </p>
                <p align="justify">A novel synthetic benchmark, named <em>CarPatch</em>, for the NeRF use case of vehicle inspection. 
                    It contains multiple-view images annotated with intrinsic and extrinsic camera parameters, 
                    the corresponding depth maps and semantic segmentation masks of car components.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/refinet.png'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://www.sciencedirect.com/science/article/pii/S0167865523000697" class="title" target="_blank"><papertitle>Depth-based 3D human pose refinement: Evaluating the refinet framework</papertitle></a>
                </p>
                <p class="pub-descr">
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>
                </p>
                <p class="pub-descr">
                    <em>Pattern Recognition Letters (PRL)</em>, 2023
                </p>
                <p class="pub-descr">
                    <a href="https://www.sciencedirect.com/science/article/pii/S0167865523000697" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/refinet.bib" target="_blank">bibtex</a>
                </p>
                <p align="justify">An unsupervised approach used to train a Transformer-based architecture that learns to detect dynamic hand gestures in a continuous temporal sequence.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/unsup_det_dhg.png' style="background-color:white;">
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-06427-2_35" class="title" target="_blank"><papertitle>Unsupervised Detection of Dynamic Hand Gestures from Leap Motion Data</papertitle></a>
                </p>
                <p class="pub-descr">
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>
                </p>
                <p class="pub-descr">
                    <em>ICIAP 2021</em> - <strong style="color: green">Poster</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-06427-2_35" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/unsup_det_dhg.bib" target="_blank">bibtex</a>
                </p>
                <p align="justify">An unsupervised approach used to train a Transformer-based architecture that learns to detect dynamic hand gestures in a continuous temporal sequence.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/shrec_2021.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://www.sciencedirect.com/science/article/pii/S0097849321001382?casa_token=k4QLrjpBJpMAAAAA:N9yiOH7FnirjW9R9JmrIx_38MwTVmTj60ZivYgTnvH2XJXcnuyG_pqz2abkkbIQ01V4c03ke" class="title" target="_blank"><papertitle>SHREC 2021: Skeleton-based hand gesture recognition in the wild</papertitle></a>
                </p>
                <p class="pub-descr">
                    Ariel Caputo, Andrea Giacchetti, Simone Soso, Deborah Pintani,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>,
                    <em>et al.</em>
                </p>
                <p class="pub-descr">
                    <em>Computers & Graphics</em>, 2021
                </p>
                <p class="pub-descr">
                    <a href="https://pdf.sciencedirectassets.com/271576/1-s2.0-S0097849321X00064/1-s2.0-S0097849321001382/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEE4aCXVzLWVhc3QtMSJGMEQCIF8BBbTZrcdYh28y%2BVPaQD1gotblRlnkF2%2BP7uDAqd87AiA0NYMmEUT42hMfOn6%2B5siz4hUdE1Y5gBf4vn0mwx07FSrSBAgXEAQaDDA1OTAwMzU0Njg2NSIMlZcTgJh0YMYyJk0%2FKq8EYiKmCoWGY6C44MgbuTCsdd37E673wMSGf9d0HabFBzU5iZRhVvpyllvWLv1b9FsFmMDHgaECzdI0%2BA5e7qI27wO51u8qkVf0sBJqWqMnbyLgCOxPHcKyiB%2BVvsSvx7OQ%2FdT2OITCFR0Z3sROcsmq85VaBsIT8GUBvWu6E%2BwQjKBhuomg5r5ISQw2NEWePJJo95TJf6ZCNrSzM5o6DEuuelMlRIhhajy40v1IJP70gHjg6Uf2tg1Aj4hYaltf%2B6OUnptuJGHi%2BKDdlxyerebsq599xDlJ54rP1zTEmJ6CZYY%2FwVTRrptb%2BPEbIWftWCF7o6MFQ%2FL%2BUPo3CksAztpInladhqRNasZaDoFczvdbczIlOKfN9AY69MKLmPmlwSQPfXL9Oti%2BAc8h4D8qqydPyDxSBBU6x60Mm2A18%2Bp5v2k3tvu4zhGQozZWKz3BABhNL%2F%2FyDlERNgsERvI%2FNdGaaTveEG5rhRxnxGf9%2B0Di1aWTqL3ufNSVwIxS37bJalTfWgaMLXC0u%2FEL7nKueaLzwYPF%2Fw0fDkUjROiIg7XGknVXO8b56YXztxi%2BN0IYZ%2B3geLa6riLk%2FgEll4w4ANq88HUErum48kMcUZNNoUSIUf7VA%2FdlWKcfwrHSrWsxCI8Aul5bLPXwfvcifXZ0JNTQEkJ%2FNz5cYjrmFNNfv0fJci3BYDPZmosduGFbyMEdeXVv%2BSc6yQOA04pt2xswy3QOoL9O9R9x0mtkJumErL4QnzCg%2FLSTBjqqAQ5%2Fs2YmfwTSMAG4QVtD1XdFydGI4XGffrOxQdGj88H4iK4TloAuY%2BphYdY9nW%2FTBzX%2Fc2iBE2M%2F0vYKK4bonviOXfm3kkTVV7pDiNTaWG%2BSyTZ7sXAqng7slA7lhylN6IVWjQet1uyOqg%2FjEH2DJHscJ9lIbgsR2m5iGPPe%2FwBzQIN%2FboydKTNvDBCx1JdAZhoBmArnIOK3xnG1CXim6Bf%2B0eC5KG4yGArz&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220430T142805Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6CYFUBXI%2F20220430%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=44ed12c41d81b4ed99aefb70792d9e73ea46e6b73e4d75b2b36c1c081d0d5f29&hash=26236665e55620f3fbea024eb00067f7e0e46cead77e8325018d65b99b52d099&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0097849321001382&tid=spdf-673cb692-a63d-433e-8e17-ee88c026031b&sid=58a4c9cb976e70408058ca49efef1f9e0746gxrqb&type=client&ua=5257575f07575c035206&rr=7040f63c4f66a319" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/shrec_2021.bib" target="_blank">bibtex</a>
                </p>
                <p align="justify">A Transformer-based architecture and a Finite State Machine (FSM) are able to detect and classify a gesture. 
                    One of the proposals in the SHREC2021 contest.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/pig_behavior.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010288405240533" class="title" target="_blank"><papertitle>Extracting Accurate Long-term Behavior Changes from a Large Pig Dataset</papertitle></a>
                </p>
                <p class="pub-descr">
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=101" target="_blank">Luca Bergamini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=38" target="_blank">Simone Calderara</a>,
                    <a href="https://pure.sruc.ac.uk/en/persons/rick-death" target="_blank">Rick B. D'Eath</a>,
                    <a href="https://homepages.inf.ed.ac.uk/rbf/" target="_blank">Robert B. Fisher</a>
                </p>
                <p class="pub-descr">
                    <em>VISAPP 2021</em> - <strong style="color: green">Poster</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://www.scitepress.org/Papers/2021/102884/102884.pdf" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/pig_behavior.bib" target="_blank">bibtex</a> &nbsp|&nbsp
                    <a href="https://homepages.inf.ed.ac.uk/rbf/PIGDATA/" target="_blank">dataset</a>
                </p>
                <p align="justify">Given a large annotated pig dataset, long-term pig behavior analysis is possible, even though estimates from individual frames can be noisy.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/transformer_dhgr.svg' style="background-color:white;">
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://ieeexplore.ieee.org/abstract/document/9320419/" class="title" target="_blank"><papertitle>A Transformer-Based Network for Dynamic Hand Gesture Recognition</papertitle></a>
                </p>
                <p class="pub-descr">
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>
                </p>
                <p class="pub-descr">
                    <em>3DV 2020</em> - <strong style="color: green">Poster</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://iris.unimore.it/bitstream/11380/1212263/1/3DV_2020.pdf" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/transformer_dhgr.bib" target="_blank">bibtex</a>
                </p>
                <p align="justify">A Transformer-based architecture that is able to recognize dynamic hand gestures exploiting information 
                    from a single active depth sensor (depth maps and surface normals).</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/multimodal_hgc.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://www.mdpi.com/2227-9709/7/3/31" class="title" target="_blank"><papertitle>Multimodal Hand Gesture Classification for the Human-Car Interaction</papertitle></a>
                </p>
                <p class="pub-descr">
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>
                </p>
                <p class="pub-descr">
                    <em>Informatics</em>, 2020
                </p>
                <p class="pub-descr">
                    <a href="https://www.mdpi.com/2227-9709/7/3/31/pdf" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/multimodal_hgc.bib" target="_blank">bibtex</a>
                </p>
                <p align="justify">A multimodal combination of CNNs whose input is represented by RGB, depth and infrared images, 
                    achieving a good level of light invariance, a key element in vision-based in-car systems.</p>
            </div>
        </div>
    </div>
<!-- OTHER paper list ends -->

    <div class="extra">
        <div class="header">Reviewing activities</div>
        <div class="extras">
            <big><u>Conferences:</u></big>
            <ul>
                <li><p>IEEE International Conference on Robotics and Automation (ICRA)</p></li>
                <li><p>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</p></li>
                <li><p>IEEE International Conference on Pattern Recognition (ICPR)</p></li>
            </ul>
            <big><u>Journals:</u></big>
            <ul>
                <li><p>IEEE Robotics and Automation Letters (RA-L)</p></li>
            </ul>
            <big><u>Workshops:</u></big>
            <ul>
                <li><p>Towards a Complete Analysis of People: From Face and Body to Clothes (T-CAP)</p></li>
                <li><p>International Workshop and Challenge on People Analysis (WCPA)</p></li>
            </ul>
        </div>
        <div class="header" >Courses and Summer Schools</div>
        <div class="extras" style="margin-bottom: 0px;">
            <ul>
            <li><p>Advanced Course on Data Science and Machine Learning - <em>ACDL 2021, Certosa di Pontignano (SI), Italy (<a href="data/ACDL2021-Simoni.pdf" target="_blank">certificate</a>)</em></p></li>
            <li><p>International Computer Vision Summer School - <em>ICVSS 2022, Scicli (RG), Italy (<a href="data/ICVSS2022-Simoni.pdf" target="_blank">certificate</a>)</em></p></li>
            <li><p>ELLIS Summer School on Large-Scale AI for Research and Industry - <em>2023, Modena (MO), Italy (<a href="data/ELLIS2023-Simoni.pdf" target="_blank">certificate</a>)</em></p></li>
            </ul>
            <br>
            <p class="source">
                <a href="https://github.com/alexj94/alexj94.github.io" target="_blank" rel="noopener noreferrer">Source code</a>
            </p>
            <p class="style_credit">
                <a href="https://jonbarron.info" target="_blank" rel="noopener noreferrer">Credit for style and layout</a>
            </p>
        </div>
    </div>

  </div>

</body>

</html>

