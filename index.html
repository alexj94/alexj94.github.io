<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Alessandro Simoni</title>
    <meta name="author" content="Alessandro Simoni">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!--<link rel="icon" type="image/png" href="my_icon.png">-->
</head>

<body>

  <div class="container">

    <div class="header">
        <img class="pic" style="border-radius:50%;" src="images/AlessandroSimoni.jpg" alt=""/>
        <div class="description">
            <div class="name">Alessandro Simoni</div>
            <p align="justify">
                I am a Ph.D. Candidate at <a href="http://imagelab.ing.unimore.it/imagelab/index.asp" target="_blank">AImageLab</a> at the University of Modena and Reggio Emilia, Italy.
            </p>
            <p align="justify">
                My research activities are focused on Computer Vision and Deep Learning applied to Collaborative Robotic tasks, more precisely in topics like 3D Object Reconstruction and Human/Robot Pose Estimation. I work under the supervision of <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Prof. Roberto Vezzani</a>.
            </p>

            <p class="contact">
                <a href="mailto:alle.simoni.as@gmail.com">Email</a> &nbsp|&nbsp
                <a href="data/AlessandroSimoni-CV.pdf" target="_blank">CV</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?user=MDGDMWIAAAAJ&hl=it" target="_blank">Google Scholar</a> &nbsp|&nbsp
                <a href="https://github.com/alexj94" target="_blank">Github</a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/alessandro-simoni/" target="_blank">LinkedIn</a>
            </p>
        </div>
    </div>

<!-- Research -->
    <div class="research">
      <div class="header">Research activities</div>
      <div class="description">
          <p align="justify">
              In the first part of my Ph.D. I tackled the task of 3D Object Reconstruction. More recently, I've started working on robotic tasks, involving 3D Pose Estimation. Representative papers are <span class="highlight">highlighted</span>.
          </p>
          <h2 style="font-weight: normal; margin: 10px 0;"><u>Authored publications:</u></h2>
      </div>
    </div>

<!-- RELEVANT paper list begins -->
    <div class="papers">

        <hr>

        <div class="paper" style="background-color: #ffffd0">
            <div class="img_default">
                <img src='images/3d_rpe.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://arxiv.org/abs/2207.02519" class="title" target="_blank"><papertitle>Semi-Perspective Decoupled Heatmaps for 3D Robot Pose Estimation from Depth Maps</papertitle></a>
                </p>
                <p class="pub-descr">
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>
                </p>
                <p class="pub-descr">
                    <em>IEEE Robotics and Automation Letters (RA-L)</em> - <strong style="color: red">Presented at IROS 2022.</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://arxiv.org/abs/2207.02519" target="_blank">arXiv</a> &nbsp|&nbsp
                    <a href="biblio/spdh.bib" target="_blank">bibtex</a> &nbsp|&nbsp
                    <a href="https://aimagelab.ing.unimore.it/go/rpe" target="_blank">webpage</a> &nbsp|&nbsp
                    <a href="https://aimagelab.ing.unimore.it/go/simba" target="_blank">dataset</a> &nbsp|&nbsp
                    <a href="https://github.com/aimagelab/rpe_spdh" target="_blank">code</a> &nbsp|&nbsp
                    <a href="data/spdh_slides.pdf" target="_blank">slides</a>
                    <a href="data/spdh_slides.pdf" target="_blank">video demo</a>
                </p>
                <p align="justify">Thanks to a novel 3D pose representation composed of two decoupled heatmaps, efficient deep networks from the 2D HPE domain can be adapted to accurately compute 3D joints locations in world coordinates. Moreover, depth maps are used to bridge the gap between synthetic and real data.</p>
            </div>
        </div>

        <hr>

        <div class="paper" style="background-color: #ffffd0">
            <div class="img_default">
                <img src='images/mcmr.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://ieeexplore.ieee.org/abstract/document/9665934" class="title" target="_blank"><papertitle>Multi-Category Mesh Reconstruction From Image Collections</papertitle></a>
                </p>
                <p class="pub-descr">
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>
                </p>
                <p class="pub-descr">
                    <em>3DV</em>, 2021 - <strong style="color: green">Poster</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://arxiv.org/abs/2110.11256" target="_blank">arXiv</a> &nbsp|&nbsp
                    <a href="biblio/mcmr.bib" target="_blank">bibtex</a> &nbsp|&nbsp
                    <a href="https://github.com/aimagelab/mcmr" target="_blank">code</a> &nbsp|&nbsp
                    <a href="data/mcmr_poster.pdf" target="_blank">poster</a> &nbsp|&nbsp
                    <a href="data/mcmr_slides.pdf" target="_blank">slides</a> &nbsp|&nbsp
                    <a href="https://slideslive.com/38972381/multicategory-mesh-reconstruction-from-image-collections?ref=" target="_blank">presentation (video)</a>
                </p>
                <p align="justify">A multi-category mesh reconstruction framework infers the textured mesh of objects, learning category-specific priors in an unsupervised manner and obtaining smooth shapes with a dynamic mesh subdivision approach.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/cmc_vkl.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://www.scitepress.org/PublicationsDetail.aspx?ID=pr1Lw6QeqOY=&t=1" class="title" target="_blank"><papertitle>Improving Car Model Classification through Vehicle Keypoint Localization</papertitle></a>
                </p>
                <p class="pub-descr">
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>
                </p>
                <p class="pub-descr">
                    <em>VISAPP</em>, 2021 - <strong style="color: red">Oral</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://www.scitepress.org/Papers/2021/102078/102078.pdf" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/cmc_vkl.bib" target="_blank">bibtex</a> &nbsp|&nbsp
                    <a href="data/cmc_vkl_slides.pdf" target="_blank">slides</a> &nbsp|&nbsp
                    <a href="data/cmc_vkl_presentation.mp4" target="_blank">presentation (video)</a>
                </p>
                <p align="justify">A multi-task framework combines visual features and keypoint localization features in order to improve car model classification accuracy.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/fus_gen.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://ieeexplore.ieee.org/abstract/document/9412880" class="title" target="_blank"><papertitle>Future Urban Scenes Generation Through Vehicles Synthesis</papertitle></a>
                </p>
                <p class="pub-descr">
                <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=101" target="_blank">Luca Bergamini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=91" target="_blank">Andrea Palazzi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=38" target="_blank">Simone Calderara</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>
                </p>
                <p class="pub-descr">
                    <em>ICPR</em>, 2020 - <strong style="color: green">Poster</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://arxiv.org/abs/2007.00323" target="_blank">arXiv</a> &nbsp|&nbsp
                    <a href="biblio/fus_gen.bib" target="_blank">bibtex</a> &nbsp|&nbsp
                    <a href="https://github.com/alexj94/future_urban_scene_generation" target="_blank">code</a> &nbsp|&nbsp
                    <a href="data/fus_gen_poster.pdf" target="_blank">poster</a> &nbsp|&nbsp
                    <a href="data/fus_gen_slides.pdf" target="_blank">slides</a> &nbsp|&nbsp
                    <a href="data/fus_gen_presentation.mp4" target="_blank">presentation (video)</a>
                </p>
                <p align="justify">A two-stage approach in which interpretable information are exploited by a novel view synthesis architecture in order to reproduce the future visual appearance of vehicles in an urban scene.</p>
            </div>
        </div>
    </div>
<!-- RELEVANT paper list ends -->

    <div class="research">
        <div class="description" style="padding: 2.5% 2.5% 2.5% 2.5%;">
            <h2 style="font-weight: normal; margin: 10px 0;"><u>Co-authored publications:</u></h2>
        </div>
    </div>

<!-- OTHER paper list begins -->

    <div class="papers">

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/unsup_det_dhg.png'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-06427-2_35" class="title" target="_blank"><papertitle>Unsupervised Detection of Dynamic Hand Gestures from Leap Motion Data</papertitle></a>
                </p>
                <p class="pub-descr">
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>
                </p>
                <p class="pub-descr">
                    <em>ICIAP</em>, 2021 - <strong style="color: green">Poster</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-06427-2_35" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="" target="_blank">bibtex</a>
                </p>
                <p align="justify">An unsupervised approach used to train a Transformer-based architecture that learns to detect dynamic hand gestures in a continuous temporal sequence.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/shrec_2021.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://www.sciencedirect.com/science/article/pii/S0097849321001382?casa_token=k4QLrjpBJpMAAAAA:N9yiOH7FnirjW9R9JmrIx_38MwTVmTj60ZivYgTnvH2XJXcnuyG_pqz2abkkbIQ01V4c03ke" class="title" target="_blank"><papertitle>SHREC 2021: Skeleton-based hand gesture recognition in the wild</papertitle></a>
                </p>
                <p class="pub-descr">
                    Ariel Caputo, Andrea Giacchetti, Simone Soso, Deborah Pintani,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>,
                    <em>et al.</em>
                </p>
                <p class="pub-descr">
                    <em>Computers & Graphics</em>, 2021
                </p>
                <p class="pub-descr">
                    <a href="https://pdf.sciencedirectassets.com/271576/1-s2.0-S0097849321X00064/1-s2.0-S0097849321001382/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEE4aCXVzLWVhc3QtMSJGMEQCIF8BBbTZrcdYh28y%2BVPaQD1gotblRlnkF2%2BP7uDAqd87AiA0NYMmEUT42hMfOn6%2B5siz4hUdE1Y5gBf4vn0mwx07FSrSBAgXEAQaDDA1OTAwMzU0Njg2NSIMlZcTgJh0YMYyJk0%2FKq8EYiKmCoWGY6C44MgbuTCsdd37E673wMSGf9d0HabFBzU5iZRhVvpyllvWLv1b9FsFmMDHgaECzdI0%2BA5e7qI27wO51u8qkVf0sBJqWqMnbyLgCOxPHcKyiB%2BVvsSvx7OQ%2FdT2OITCFR0Z3sROcsmq85VaBsIT8GUBvWu6E%2BwQjKBhuomg5r5ISQw2NEWePJJo95TJf6ZCNrSzM5o6DEuuelMlRIhhajy40v1IJP70gHjg6Uf2tg1Aj4hYaltf%2B6OUnptuJGHi%2BKDdlxyerebsq599xDlJ54rP1zTEmJ6CZYY%2FwVTRrptb%2BPEbIWftWCF7o6MFQ%2FL%2BUPo3CksAztpInladhqRNasZaDoFczvdbczIlOKfN9AY69MKLmPmlwSQPfXL9Oti%2BAc8h4D8qqydPyDxSBBU6x60Mm2A18%2Bp5v2k3tvu4zhGQozZWKz3BABhNL%2F%2FyDlERNgsERvI%2FNdGaaTveEG5rhRxnxGf9%2B0Di1aWTqL3ufNSVwIxS37bJalTfWgaMLXC0u%2FEL7nKueaLzwYPF%2Fw0fDkUjROiIg7XGknVXO8b56YXztxi%2BN0IYZ%2B3geLa6riLk%2FgEll4w4ANq88HUErum48kMcUZNNoUSIUf7VA%2FdlWKcfwrHSrWsxCI8Aul5bLPXwfvcifXZ0JNTQEkJ%2FNz5cYjrmFNNfv0fJci3BYDPZmosduGFbyMEdeXVv%2BSc6yQOA04pt2xswy3QOoL9O9R9x0mtkJumErL4QnzCg%2FLSTBjqqAQ5%2Fs2YmfwTSMAG4QVtD1XdFydGI4XGffrOxQdGj88H4iK4TloAuY%2BphYdY9nW%2FTBzX%2Fc2iBE2M%2F0vYKK4bonviOXfm3kkTVV7pDiNTaWG%2BSyTZ7sXAqng7slA7lhylN6IVWjQet1uyOqg%2FjEH2DJHscJ9lIbgsR2m5iGPPe%2FwBzQIN%2FboydKTNvDBCx1JdAZhoBmArnIOK3xnG1CXim6Bf%2B0eC5KG4yGArz&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220430T142805Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6CYFUBXI%2F20220430%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=44ed12c41d81b4ed99aefb70792d9e73ea46e6b73e4d75b2b36c1c081d0d5f29&hash=26236665e55620f3fbea024eb00067f7e0e46cead77e8325018d65b99b52d099&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0097849321001382&tid=spdf-673cb692-a63d-433e-8e17-ee88c026031b&sid=58a4c9cb976e70408058ca49efef1f9e0746gxrqb&type=client&ua=5257575f07575c035206&rr=7040f63c4f66a319" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/shrec_2021.bib" target="_blank">bibtex</a>
                </p>
                <p align="justify">A Transformer-based architecture and a Finite State Machine (FSM) are able to detect and classify a gesture. One of the proposals in the SHREC2021 contest.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/pig_behavior.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010288405240533" class="title" target="_blank"><papertitle>Extracting Accurate Long-term Behavior Changes from a Large Pig Dataset</papertitle></a>
                </p>
                <p class="pub-descr">
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=101" target="_blank">Luca Bergamini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=38" target="_blank">Simone Calderara</a>,
                    <a href="https://pure.sruc.ac.uk/en/persons/rick-death" target="_blank">Rick B. D'Eath</a>,
                    <a href="https://homepages.inf.ed.ac.uk/rbf/" target="_blank">Robert B. Fisher</a>
                </p>
                <p class="pub-descr">
                    <em>VISAPP</em>, 2021 - <strong style="color: green">Poster</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://www.scitepress.org/Papers/2021/102884/102884.pdf" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/pig_behavior.bib" target="_blank">bibtex</a> &nbsp|&nbsp
                    <a href="https://homepages.inf.ed.ac.uk/rbf/PIGDATA/" target="_blank">dataset</a>
                </p>
                <p align="justify">Given a large annotated pig dataset, long-term pig behavior analysis is possible, even though estimates from individual frames can be noisy.</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/unsup_det_dhg.png'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://ieeexplore.ieee.org/abstract/document/9320419/" class="title" target="_blank"><papertitle>A Transformer-Based Network for Dynamic Hand Gesture Recognition</papertitle></a>
                </p>
                <p class="pub-descr">
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>
                </p>
                <p class="pub-descr">
                    <em>3DV</em>, 2020 - <strong style="color: green">Poster</strong>
                </p>
                <p class="pub-descr">
                    <a href="https://iris.unimore.it/bitstream/11380/1212263/1/3DV_2020.pdf" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/transformer_dhgr.bib" target="_blank">bibtex</a>
                </p>
                <p align="justify">A Transformer-based architecture that is able to recognize dynamic hand gestures exploiting information from a single active depth sensor (depth maps and surface normals).</p>
            </div>
        </div>

        <hr>

        <div class="paper">
            <div class="img_default">
                <img src='images/multimodal_hgc.svg'>
            </div>
            <div class="description">
                <p class="pub-descr">
                    <a href="https://www.mdpi.com/2227-9709/7/3/31" class="title" target="_blank"><papertitle>Multimodal Hand Gesture Classification for the Human-Car Interaction</papertitle></a>
                </p>
                <p class="pub-descr">
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=116" target="_blank">Andrea D'Eusanio</a>,
                    <strong>Alessandro Simoni</strong>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=104" target="_blank">Stefano Pini</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=87" target="_blank">Guido Borghi</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=8" target="_blank">Roberto Vezzani</a>,
                    <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1" target="_blank">Rita Cucchiara</a>
                </p>
                <p class="pub-descr">
                    <em>Informatics</em>, 2020
                </p>
                <p class="pub-descr">
                    <a href="https://www.mdpi.com/2227-9709/7/3/31/pdf" target="_blank">paper</a> &nbsp|&nbsp
                    <a href="biblio/multimodal_hgc.bib" target="_blank">bibtex</a>
                </p>
                <p align="justify">A multimodal combination of CNNs whose input is represented by RGB, depth and infrared images, achieving a good level of light invariance, a key element in vision-based in-car systems.</p>
            </div>
        </div>
    </div>
<!-- OTHER paper list ends -->

    <div class="extra">
        <div class="header" >Reviewing Service</div>
        <div class="extras">
            <big><u>Conferences:</u></big>
            <ul>
                <li><p>IEEE International Conference on Pattern Recognition (ICPR)</p></li>
            </ul>
            <big><u>Journals:</u></big>
            <ul>
                <li><p>IEEE Robotics and Automation Letters (RA-L)</p></li>
            </ul>
            <big><u>Workshops:</u></big>
            <ul>
                <li><p>Towards a Complete Analysis of People: From Face and Body to Clothes (T-CAP)</p></li>
                <li><p>International Workshop and Challenge on People Analysis (WCPA)</p></li>
            </ul>
        </div>
        <div class="header" >Courses and Summer Schools</div>
        <div class="extras" style="margin-bottom: 0px;">
            <ul>
            <li><p>Advanced Course on Data Science and Machine Learning - <em>ACDL 2021, Certosa di Pontignano (SI), Italy (<a href="data/ACDL2021-Simoni.pdf" target="_blank">certificate</a>)</em></p></li>
            <li><p>International Computer Vision Summer School - <em>ICVSS 2022, Scicli (RG), Italy (<a href="data/ICVSS2022-Simoni.pdf" target="_blank">certificate</a>)</em></p></li>
            </ul>
            <br>
            <p class="source">
                <a href="https://github.com/alexj94/alexj94.github.io" target="_blank" rel="noopener noreferrer">Source code</a>
            </p>
            <p class="style_credit">
                <a href="https://jonbarron.info" target="_blank" rel="noopener noreferrer">Credit for style and layout</a>
            </p>
        </div>
    </div>

  </div>

</body>

</html>

